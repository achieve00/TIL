# 2025-03-27 TIL

## 1. OS 강의


## 2. 코딩 테스트(이코테)


## 3. SQL 강의


## 4. LLM 수업 복습

### Inductive bias
- 모델이 학습 데이터에 없는 부분을 어떻게 일반화(generalize)할 것인지를 결정하는 '사전 가정' 혹은 '구조적 제약'
    - CNN: 지역성(locality)
    - RNN: 순차성(sequential)
    - Transformer (attention 기반): 약한 inductive bias → **데이터 많이 필요**
- 일부 문헌에서는 *Learning bias*라고도 부름

```text
Transformer (attention 기반)
│
├─ 범용성, 유연성 높음
├─ Inductive bias 약함 → 오버피팅 위험
└─ GNN처럼 명시적 그래프는 없지만
    └─ Attention Score로 관계 soft하게 모델링
```

### Vanilla Transformers
#### 기존 모델의 문제점
1. Model efficiency → 문장이 2배 길어지면 Self-Attention 연산량이 4배 증가 (O(n²))
2. Model generalization → 구조적 inductive bias가 적기 때문에, 작은 데이터셋에서 일반화가 어렵고 과적합 위험
3. Model adaptation → 특정 downstream task (예: 질의응답, 감정 분석 등)에 적용하려면, 추가적인 fine-tuning, 구조 수정, 연산 효율 조정이 필요

#### Scaled Dot Product Attention
$$Attention(Q, K, V) = softmax(QKᵀ / √d) · V$$
- 내적 값 커지면 softmax 치우침 → √d로 scaling

#### Multi-head Attention
- 입력을 여러 head로 분할 → 다양한 관점의 attention 병렬 계산
- 총 차원 유지 (예: 512 = 64 × 8)

#### Attention 종류
1. Self-attention       → Q, K, V 모두 동일
2. Masked self-attention→ 미래 토큰을 보지 않도록 어텐션 마스크를 적용한 self-attention
3. Cross-attention      → 디코더가 인코더 출력 참조

#### 연산 복잡도 분석
Self-Attention:   O(T²D)    ← 병목 원인
FFN (Feedforward): O(TD²)

T: 시퀀스 길이, D: 임베딩 차원

### Sparse Attention
#### Motivation
- 모든 쌍 계산(X) → 일부 중요한 쌍만 선택
    - 계산 효율성 향상 + 구조적 inductive bias 부여

#### Position-based Patterns
1. Global       → 특정 토큰 ↔ 전체
2. Band (Local) → 인접 토큰만
3. Dilated      → 간격 띄운 연결 (long-range)
4. Random       → 일부 무작위 연결
5. Block-local  → 블록 단위 attention

#### Compound 구조
- Star-Transformer       = Band + Global
- Longformer             = Band + Dilated + Internal Global
- BigBird                = Band + Global + Random
- ETC (Extended)         = Band + External Global

### Linearized Attention
#### Key Idea
$$softmax(QKᵀ) ≈ φ(Q) · (φ(K)ᵀ · V)$$
→ softmax 생략, kernel 기반 근사
→ 기존 O(T²D) → O(TrD)로 연산량 감소


### Attention with Prior
Attention weight를 QK 내적이 아닌 사전 지식(prior) 기반으로 결정하거나 보조
- Gaussian Prior     → 거리 기반 가중치
- Syntax Prior       → 구문 구조 기반
- Only Prior         → QK 없이 prior만 사용

#### Synthesizer
Attention score = MLP(Q) or 고정 패턴
→ QKᵀ 생략

- Dense Synthesizer  → MLP(Q)
- Random Synthesizer → 고정된 랜덤 행렬 사용

### Position Embedding
- APE: Absolute, 위치 벡터를 입력에 더함
- RPE: Relative, 토큰 간 거리(i-j)로 위치 정보 부여

### Layer Normalization
- Post-LN: 원래 구조 (Residual 뒤에 LN) → 학습 불안정
- Pre-LN: LN 먼저 적용 → 안정적 gradient 흐름

### ReZero

$$x_l+1 = x_l + α · F(x_l)    (α는 학습 파라미터, 초기 0)$$
→ 초기에는 거의 identity, 점점 깊어짐
→ LayerNorm 없이도 안정적 학습

### Searching Activation Functions in FFN
- Swish: x · sigmoid(βx) → 부드러운 ReLU
- GELU: 확률 기반 soft gating → GPT 계열 사용
- Mixture of Experts (MoE)
    - 여러 FFN 중 일부만 선택 (gating function)→ 연산 효율 + 표현력 모두 확보
- Switch Transformer
    - MoE의 단순화 버전  → 입력마다 하나의 expert만 사용 (1-of-N 선택)

## Reflection